#!/usr/bin/env python
"""
eval_lora.py  â”€  Batchâ€‘evaluate a LoRAâ€‘tuned Qwen3â€‘32B on TIâ€‘ONE

â€¢ å…³é—­å¹³å°æ³¨å…¥çš„åˆ†å¸ƒå¼ / å¼ é‡å¹¶è¡Œç¯å¢ƒå˜é‡ï¼ˆå•è¿›ç¨‹æ¨ç†ï¼‰
â€¢ é€æ¡ç”Ÿæˆæµ‹è¯•é›†ï¼Œä¿å­˜é¢„æµ‹ JSONL
â€¢ æ‰“å° SacreBLEU ä¸ ROUGEâ€‘Lâ€‘F1
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç¯å¢ƒå˜é‡å±è”½ â€”â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
for v in ("RANK", "WORLD_SIZE", "LOCAL_RANK"):
    os.environ.pop(v, None)                     # æ“¦æ‰å¯èƒ½æ³¨å…¥çš„ DDP å˜é‡
os.environ["ACCELERATE_DISABLE_DISTRIBUTED"] = "1"
os.environ["TRANSFORMERS_NO_TP"] = "1"         # ç¦ç”¨ HF tensor parallel

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import json, argparse, tqdm, numpy as np, sacrebleu, torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI / è·¯å¾„å‚æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
p = argparse.ArgumentParser()
p.add_argument("--base_dir",   required=True, help="Full Qwen3â€‘32B base weights")
p.add_argument("--lora_key",   required=True, help="Directory with adapter_model.bin")
p.add_argument("--test_file",  required=True, help="Alpacaâ€‘format test.jsonl")
p.add_argument("--output_dir", required=True, help="Where to write metrics & preds")
args = p.parse_args()

PRED_FILE = os.path.join(args.output_dir, "test_predictions.jsonl")
os.makedirs(args.output_dir, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Load model + LoRA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ”„ Loading tokenizer / base / LoRA â€¦")
tokenizer = AutoTokenizer.from_pretrained(args.lora_key, trust_remote_code=True, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(
    args.base_dir,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
model = PeftModel.from_pretrained(
    model, 
    args.lora_key, 
    is_trainable=False,
    local_files_only=True,
)
model.eval()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Evaluate loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
refs, hyps = [], []
with open(args.test_file, encoding="utf-8") as fin, \
     open(PRED_FILE, "w", encoding="utf-8") as fout:

    for line in tqdm.tqdm(fin, desc="âœ“ generating"):
        ex = json.loads(line)
        prompt = f"{ex['instruction']}\n### è¾“å…¥:\n{ex['input']}\n### è¾“å‡º:\n"
        in_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
        print(f"Current Source Text: {ex['input']}")

        with torch.no_grad():
            out_ids = model.generate(
                in_ids,
                max_new_tokens=1024,
                temperature=0.7,
                top_p=0.9,
            )
        pred = tokenizer.decode(out_ids[0][in_ids.shape[1]:], skip_special_tokens=True)
        print(f"Current Translation: {pred}")

        refs.append(ex["output"].strip())
        hyps.append(pred.strip())

        json.dump(
            {
                "instruction": ex["instruction"],
                "input": ex["input"],
                "reference": ex["output"],
                "prediction": pred,
            },
            fout,
            ensure_ascii=False,
        )
        fout.write("\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
bleu   = sacrebleu.corpus_bleu(hyps, [refs]).score
# rl_f   = np.mean([s["rouge-l"]["f"] for s in rouge.get_scores(hyps, refs)]) * 100

print("\n=================  EVAL RESULT  =================")
print(f"SacreBLEU   : {bleu:6.2f}")
print(f"Predictions saved to â†’  {PRED_FILE}")
print("=================================================\n")