#!/usr/bin/env python
"""
eval_base.py â”€ Evaluate *vanilla* Qwen3â€‘32B (no LoRA) on the same test set.

â€¢ Singleâ€‘process, layerâ€‘parallel across GPUs
â€¢ Generates translations, writes JSONL identical to eval_lora.py
â€¢ Prints SacreBLEU for sideâ€‘byâ€‘side comparison
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Disable platformâ€‘injected distributed vars â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
for v in ("RANK", "WORLD_SIZE", "LOCAL_RANK"):
    os.environ.pop(v, None)
os.environ["ACCELERATE_DISABLE_DISTRIBUTED"] = "1"
os.environ["TRANSFORMERS_NO_TP"] = "1"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import json, argparse, tqdm, sacrebleu
import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI arguments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
p = argparse.ArgumentParser()
p.add_argument("--base_dir",   required=True, help="Full Qwen3â€‘32B base weights")
p.add_argument("--test_file",  required=True, help="Alpacaâ€‘format test.jsonl")
p.add_argument("--output_dir", required=True, help="Where to write metrics & preds")
args = p.parse_args()

PRED_FILE = os.path.join(args.output_dir, "test_predictions_base.jsonl")
os.makedirs(args.output_dir, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Load base model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ”„ Loading tokenizer & base model â€¦")
tokenizer = AutoTokenizer.from_pretrained(
    args.base_dir, trust_remote_code=True, local_files_only=True
)
tokenizer.use_default_system_prompt = False   # å…³é—­é»˜è®¤æ¨¡æ¿ï¼Œæ‰‹åŠ¨æ„é€ 

def build_prompt(src_zh: str) -> str:
    """ä½¿ç”¨å®˜æ–¹ chat_template ç”Ÿæˆ prompt"""
    messages = [
        {
            "role": "system",
            "content": (
                "ä½ æ˜¯ä¸€ä¸ªæ¸¸æˆã€Šé»‘ç¥è¯ï¼šæ‚Ÿç©ºã€‹çš„ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ç»™å®šçš„ä¸­æ–‡æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ã€‚"
                "å¯¹äºå°–æ‹¬å·<>æˆ–å¤§æ‹¬å·{{}}åŒ…å›´çš„å ä½ç¬¦ï¼Œè¯·ä¿æŒè‹±æ–‡éƒ¨åˆ†ä¸å˜ï¼Œä»…ç¿»è¯‘å…¶ä¸­çš„ä¸­æ–‡éƒ¨åˆ†ã€‚"
                "â—åªè¾“å‡ºè‹±æ–‡è¯‘æ–‡ï¼Œä¸¥ç¦è¾“å‡ºä»»ä½•è§£é‡Šã€æ ‡æ³¨ã€<think> æ€è€ƒã€ä»£ç æˆ–å¤šä½™æ–‡æœ¬ã€‚"
            ),
        },
        {"role": "user", "content": src_zh},
    ]
    return tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )  # ç»“å°¾è‡ªåŠ¨å¸¦ <|im_start|>assistant\n

model = AutoModelForCausalLM.from_pretrained(
    args.base_dir,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True,
)
model.eval()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Evaluation loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
refs, hyps = [], []
with open(args.test_file, encoding="utf-8") as fin, \
     open(PRED_FILE, "w", encoding="utf-8") as fout:

    for line in tqdm.tqdm(fin, desc="âœ“ generating (base)"):
        ex = json.loads(line)
        prompt = build_prompt(ex["input"])
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
        print(f"Current Source Text: {ex['input']}")

        with torch.no_grad():
            out_ids = model.generate(
                input_ids,
                max_new_tokens=1024,
                temperature=0.2,      # æ›´ç¡®å®š
                top_p=0.95,
                repetition_penalty=1.05,
                eos_token_id=tokenizer.convert_tokens_to_ids("<|im_end|>"),
                pad_token_id=tokenizer.eos_token_id,
            )

        raw = tokenizer.decode(out_ids[0][input_ids.size(1):], skip_special_tokens=False)
        # print("RAW >>>", repr(raw))        # è°ƒè¯•

        raw  = re.sub(r"<think>.*?</think>", "", raw, flags=re.S)
        pred = raw.split("<|im_end|>")[0].strip()
        print(f"Current Translation: {pred}")

        refs.append(ex["output"].strip())
        hyps.append(pred.strip())

        json.dump(
            {
                "instruction": ex["instruction"],
                "input": ex["input"],
                "reference": ex["output"],
                "prediction": pred,
            },
            fout,
            ensure_ascii=False,
        )
        fout.write("\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
bleu = sacrebleu.corpus_bleu(hyps, [refs]).score

print("\n=================  BASE MODEL RESULT  ================")
print(f"SacreBLEU   : {bleu:6.2f}")
print(f"Predictions saved to â†’  {PRED_FILE}")
print("======================================================\n")
