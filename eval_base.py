#!/usr/bin/env python
"""
eval_base.py â”€ Evaluate *vanilla* Qwen3â€‘32B (no LoRA) on the same test set.

â€¢ Singleâ€‘process, layerâ€‘parallel across GPUs
â€¢ Generates translations, writes JSONL identical to eval_lora.py
â€¢ Prints SacreBLEU for sideâ€‘byâ€‘side comparison
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Disable platformâ€‘injected distributed vars â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
for v in ("RANK", "WORLD_SIZE", "LOCAL_RANK"):
    os.environ.pop(v, None)
os.environ["ACCELERATE_DISABLE_DISTRIBUTED"] = "1"
os.environ["TRANSFORMERS_NO_TP"] = "1"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import json, argparse, tqdm, sacrebleu
from transformers import AutoTokenizer, AutoModelForCausalLM

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI arguments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
p = argparse.ArgumentParser()
p.add_argument("--base_dir",   required=True, help="Full Qwen3â€‘32B base weights")
p.add_argument("--test_file",  required=True, help="Alpacaâ€‘format test.jsonl")
p.add_argument("--output_dir", required=True, help="Where to write metrics & preds")
args = p.parse_args()

PRED_FILE = os.path.join(args.output_dir, "test_predictions_base.jsonl")
os.makedirs(args.output_dir, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Load base model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ðŸ”„ Loading tokenizer & base model â€¦")
tokenizer = AutoTokenizer.from_pretrained(
    args.base_dir, trust_remote_code=True, local_files_only=True
)
model     = AutoModelForCausalLM.from_pretrained(
    args.base_dir,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True,
)
model.eval()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Evaluation loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
refs, hyps = [], []
with open(args.test_file, encoding="utf-8") as fin, \
     open(PRED_FILE, "w", encoding="utf-8") as fout:

    for line in tqdm.tqdm(fin, desc="âœ“ generating (base)"):
        ex = json.loads(line)
        prompt = f"{ex['instruction']}\n### è¾“å…¥:\n{ex['input']}\n### è¾“å‡º:\n"
        in_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
        print(f"Current Source Text: {ex['input']}")

        with model.no_sync():              # no grad
            out_ids = model.generate(
                in_ids,
                max_new_tokens=1024,       # keep same as eval_lora
                temperature=0.7,
                top_p=0.9,
            )
        pred = tokenizer.decode(out_ids[0][in_ids.shape[1]:], skip_special_tokens=True)
        print(f"Current Translation: {pred}")

        refs.append(ex["output"].strip())
        hyps.append(pred.strip())

        json.dump(
            {
                "instruction": ex["instruction"],
                "input": ex["input"],
                "reference": ex["output"],
                "prediction": pred,
            },
            fout,
            ensure_ascii=False,
        )
        fout.write("\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
bleu = sacrebleu.corpus_bleu(hyps, [refs]).score

print("\n=================  BASE MODEL RESULT  ================")
print(f"SacreBLEU   : {bleu:6.2f}")
print(f"Predictions saved to â†’  {PRED_FILE}")
print("======================================================\n")
